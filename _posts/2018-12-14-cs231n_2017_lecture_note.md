---
layout: post
title: cs231n, 11 Detection and segmentation (note)
data: 2018-09-29 01:33:00 +0900
catagories: dev
---
## **cs231n** **#11** Detection and Segmentation

# Segmentation
1. Segmentation 을 하는 작은 예로써, sliding window를 만들어 조금씩 옮겨가며 crop 된 이미지의 classification을 통해 중심점의 Category를 예측하는 방법을 생각해 볼 수 있다.

    > 단점

    computation이 너무 expensive 하다.

2. 두번째로 CNN Layer 여러개를 사용하여 Category를 한번에 모든 픽셀에 대해 분류해보는 것을 생각해 볼 수 있다.
    > Q. Loss function은 어떻게 계산하는가?
    
    every pixel 마다 ground-truth에 대한 cross-entropy loss를 계산한다. space 혹은 mini-batch 에 대한 Sum 혹은 avarage 로 계산한다.

    > Q. catagory를 이미 알고 있다고 가정하는가?
        
    그렇다. 10~20개 정도의 카테고리 분류 문제라고 생각하면 된다.

    >단점

    2번째 방법은 parameter를 잘 맞춘다면 비교적 잘 작동할 것이지만, 문제는 많은 input 크기(spacial size)를 CNN Layer 마다 계속해서 일정하게 고정해야한다는 것이다. 요즈음 카메라의 성능이 좋은 관계로 high resolution input에 대해 주로 많이 사용하는 64, 128, 256개수의 필터를 사용하여  layer sequence 연산을 하는것은 많은 비용과(super super expensive) 많은 메모리가(tones of memory) 든다.

3. 다운 샘플링후, 업샘플링을 하는것. (1/4 정도)

    레이어를 거치면서 1/2 을 두번 적용하여 1/4로 레이어를 줄인뒤 다시 키워서 한번에 모든 픽셀의 카테고리를 분류하는 방법이다.

    > Upsampling은 무엇을 의미하는가?

    unpooling을 하는 것. 여기에는 다양한 전략이 있다.
    
    (Nearest Neighbor | Bed of nails | Max unpooling)

    > **Max Unpooling**
    
    Max unpooling은 Max pooling을 했던 그 자리를 기억했다가 upsampling에 같은 자리를 사용하는 것이다.

    > Q. 왜 Max unpooling이 좋은 아이디어 인가?

    잃어버린 spcial feature들을 보강해주기 떄문이다. (help us to handle information, preserve loss from max pooling)

    > Q. 이것이 Backprop을 쉽게 하는가?
    
    많은 계산을 필요로 하지 않으므로, dynamic한 변화가 없을 것이라고 예상한다.

    > (Learnable Upsampling) Transpose Convolution

    3x3 CNN filter를 내적하면 9번의 곱셈 통해 1개의 weight이 나온다. 그 반대의 개념으로, 하나의 weight을 주면 3x3 크기의 값이 나오는데, 겹치는 영역을 고려하여 모든 필터의 합을 구해 output을 만드는 방법이다.

    일반적인 **convolution** 의 반대 개념으로, **Deconvolution** (신호처리에서 보면 잘못된 이름이나 가끔 논문에서 나온다고한다) **Upconvolution**, **Fractionally strided convolution** (input과 output의 ratio 비율을 stride가 결정한다는 뜻에서), **Backward strided convolution** (foward의 반대말 개념) 등으로 쓰인다.

    > Transpose라는 이름은 어디에서 왔는가?
    $$\begin{pmatrix}
    \vec{x}*\vec{a}=X\vec{a}
    \end{pmatrix}$$
    $$\begin{pmatrix}
    x & y & z & 0 & 0 & 0 \\
    0 & x & y & z & 0 & 0 \\
    0 & 0 & x & y & z & 0 \\
    0 & 0 & 0 & x & y & z \\
    \end{pmatrix} \begin{pmatrix}
    0 \\ a \\ b \\ c \\ d \\ 0
    \end{pmatrix} = \begin{pmatrix}
    ay + bz \\ ax + by + cz \\ bx + cy + dz \\ cx + dy
    \end{pmatrix}outputsize:4$$
    $$\begin{pmatrix}
    \vec{x}*^T\vec{a}=X^T\vec{a}
    \end{pmatrix}$$
    $$\begin{pmatrix}
    x&0&0&0\\
    y&x&0&0\\
    z&y&x&0\\
    0&z&y&x\\
    0&0&z&y\\
    0&0&0&z\\
    \end{pmatrix}\begin{pmatrix}
    a\\b\\c\\d
    \end{pmatrix}=
    \begin{pmatrix}
    ax\\ay+bx\\az+by+cx\\bz+cy+dx\\dz+dy\\dy
    \end{pmatrix}outputsize: 6
    $$

    위의 1D **1stride** example처럼 transpose conv가 stride conv와 같다. **2stride**의 경우에는 더 이상 normal convolution이 아니다. (이해 못함) 그래서 이와같은 네이밍이 지어진것 같다.
    
    $$\begin{pmatrix}
    x&y&z&0&0&0\\
    0&0&x&y&z&0
    \end{pmatrix}\begin{pmatrix}
    0\\a\\b\\c\\d\\0\\
    \end{pmatrix}=\begin{pmatrix}
    ay+bz\\bx+cy+dz
    \end{pmatrix}stride:2$$
    $$\begin{pmatrix}
    x&0\\
    y&0\\
    z&x\\
    0&y\\
    0&z\\
    0&0\\
    \end{pmatrix}\begin{pmatrix}
    a\\b\\
    \end{pmatrix}=\begin{pmatrix}
    ax\\ay\\az+bx\\by\\bz\\0
    \end{pmatrix}$$

    > Q. 왜 Avarage 하지 않고 Sum을 하나요?

    합산을 하는 이유는 공식 떄문입니다. transpose folmula에 근거를 두기 때문에 더합니다. 그러나 학생이 잘 짚은 것처럼 최종 Magnitude가 얼마나 많은 겹치는 영역이 있냐에 따라서 달라지기 때문에, 따라서 최근 논문에서는 4x4에 2 stride 혹은 2x2 에 2stride와 같은 방법을 사용하여 문제를 우회하려고 노력하는 경향이 있습니다.

    > Q. Stride Half Convolution 이 뭔가요?, 용어가 어디에서 왔나요?

    연사의 논문에서 왔습니다. 연사가 논문을 쓸 당시 Fraction conv 의 용어를 사용하였으나, 지금 와서는 transpose 라는 용어가 더 정확하다고 생각합니다.

# Classsification + Localization

Pixel Classification (Segmentation)을 하고 나면, bounding box를 그릴 수 있을 것으로 보여집니다. 같은 라벨링된 객체에 박스를 그리면 되기 떄문입니다.

실제 예시 구조에서는 이미지를 AlexNet에 넣고나서 classification이 끝나면 원래의 FCN에는 class를 예측하고, 다른 FCN에는 4개의 파라메터를 (bounding box) 나오게학습합니다. 즉, 이 구조에서는 2개의 다른 Loss가 제공됩니다.

> Q. Loss 2개를 같이 학습 하는 것이 좋은 아이디어인지 궁금합니다.

같이 하는게 일반적아지는 않습니다. 학습초반에 하나가 완전히 틀린데 비해, 다른 하나는 맞는 경우가 많이 생길경우,우리가 원하는 방향에서 완전히 빗나갈수 있기 때문이죠. 그렇지 않은 경우는 잘 학습이 될 수 도 있습니다. 이런 경우를 미연에 방지하고자, 좀 더 나아간 논문에서는 카테고리마다 제각각의 박스를 예측하게 하여, 해당 카테고리의 box loss만을 사용하고 있습니다.

> Q. 둘중에 한개의 loss가 우세한가요? 분배는 어떻게 되나요?

이런 loss를 multi-task loss 라고 합니다. 두개를 같이 줄이는 것이 목적인데, 보통 이를 각각 점수를 주어 조절합니다. (weighted sum)

weighting parameter는 다른 hyperparameter과 다르게 직접 loss에 영향을 미치기 때문에, 정하기가 꽤 어렵습니다. 문제마다 적용하여 알아봐야합니다. 이때 loss자체보다는 cross-validation 수치를 통해 조절하는게 좋습니다.

> Q. 왜 두개로 나누어 각각 학습 하지 않나요?

가끔 그렇게 하기도 합니다만, 일반적으로 transfer learning에서는 hyperparameter을 잘 맞췄을 경우에 항상 더 좋은 결과가 나오기 때문에, 그렇게 합니다. 그것은 두개의 task에 miss-match가 있기 때문일 거라 생각합니다. 실제로는 두 모델의 수렴을 잘 조절한후, 두개를 조합하여 사용하는 경우도 있긴합니다.


> Humen Pose Estimation

14개의 joint를 찾아서 계산하는 이경우 역시, 각 부위를 segmentation하여 찾는 경우에 classification + localization 문제에 속하게 됩니다. 나중에 다시 설명하겠습니다. 더불어서 거리에 대한 regrestion을 사용해야 합니다.

> Q. regrestion은 보통 뭘 말하나요?

보통 L2 유클리디안을 말합니다. 이는 연속성(continuous)을 위해 사용하는 것으로서, 카테고리화와 연속성중에서 선택할 때, joint는 연속성에 가까운 문제이기 때문에 이를 사용하는 것입니다. 기본 regrestion은 L2를 기본으로 사용합니다. 좀더 일찍 정확하게 짚고 넘어가지 않아 미안합니다.

# Detection

    Detection은 CV에서 궁극적으로 해결하려고 애써온 분야중 하나입니다. 역사를 말하자면 한 세미나동안 해도 될 정도로 많은 양입니다. 일반적으로 classification + localization 문제와 Detection 문제는 많이 다릅니다. 전자는 한 이미지에 두세개의 물체가 있다고 가정하고, 그것의 위치를 알아내는데 비해서, Deteciton은 한 장에 얼마나 많은 물체가 있는지 모릅니다.
    
    Object Detection 역시 딥러닝의 중요문제가 해결된 이후 정확도가 계속 상승하고 있습니다. 주로 PASCAL VOC가 정확도 측정에 사용되었습니다. 2015년 이후 비교적 쉬운문제로 분류되어 비공식적인 발표가 많아, 공식적인 연도별 정확도 통계가 집계되어 있지는 않지만, 주로 80% 이상의 높은 정확도 성능을 보여주고 있습니다.

### 1. Sliding Window 방식(brute-force)
Object의 output이 몇개인지 정확히 모르므로, 우선 가장 간단하게 Sliding Window 방식을 이용하여 적용해 볼 수 있습니다. 적당한 사각형을 골라 이미지를 순회하면서 Object인지 아닌지 판단합니다.

> 단점

crop을 결정하는 방법이 너무 다양합니다. 많은 수의 crop이 존재하고, 이를 CNN을 한번씩 돌리기 때문에, 계산적으로 너무 무겁고 비용이 많이 들게 됩니다.

### 2. Region Proposals (fix algorithm)

물체가 어디에 존재할지에 대한 대강의 추측을 하는 알고리즘을 넣어서, 한 이미지당 몇 천개의 box를 제공합니다. 이 박스는 엣지나 컬러에 의해서 결정됩니다. 비교적 빠르면서 2초만에 cpu에서 작동하고, 2천개의 박스를 주는 알고리즘이 주로 사용됩니다.

이 방법은 R-CNN에서 사용되어 널리 알려지게 되었습니다. Regions of Interest를 구하여 ~2K 정도의 사각형 하나하나를 CNN에 돌리게 됩니다. 

> R-CNN

Region Proposals 알고리즘 이후, 각각을 CNN에 넣고, SVM 를 이용하여 물체를 분류한 뒤, 다시 RP에서 제출된 bouding box를 수정하는 과정을 거치게 됩니다.
(RP-CNN - SVMs - correction bouding box)

이는 역시 classificaiton-localization 에서 만나보았던 multi-task loss를 가지고 있습니다. RP 단계에서 object가 틀리는지, 맞는지에 대한 loss, RP 단계에서 box에 대한 정확도, 그리고 SVM의 분류 정확도와 다시 계산된 최종 bounding box에 대한 정확도, 총 4가지 입니다.

> Q. 사각형의 aspect ratio는 어떻게 계산하나요?

R-CNN 논문에서 많은 파라메터를 써서 계산을 했습니다. 일반적인 해결책은 연사가 제시할 수 없겠습니다.

> Q. 꼭 ROI가 사각형이어야 하나요?

이미지를 주로 사각형으로 줄이기 떄문이지만, segmentaion을 이용한다면 아닐수도 있습니다.

> Q. R-CNN은 학습하는 알고리즘 인가요?

아뇨. RP는 고정된 알고리즘입니다

> Q. RP가 제시하는 offset 좌표는 항상 ground-trues의 ROI 안(inside)이어야 하나요??

아닙니다. 예를 들어 RP단계에서 사람을 찾았으나, 머리가 없이 박스가 쳐질 수 있습니다. 따라서,  마지막 단계에서 고치는 알고리즘이 적용되었습니다. 즉, ROI 밖을 검출 할 수도 있습니다.

> Q.  오브젝트가 아닌것도 예측해야하는가?

그렇다 background라는 카테고리는 여기 오브젝트가 없다고 알려주는 카테고리이다.

> Q. 어떤 데이터가 필요한가요?

이미지는 object box에 대한 정보, 그리고 물체의 label과 segmentation이 모두 되어있어야합니다.

> 단점

RP를 썼지만, 고정 알고리즘이고, CNN을 각각 따로 돌리기 때문에 굉장히 시간이 오래걸리는 작업이고 계산량이 많다.(train과 test모두 오래걸린다.)

### 3. Region Proposals (Network)
기존 RCNN은 학습하는데 있어서, 100GB의 feature와 param이 필요하고, 따라서 엄청 오래 걸린다.
(super super slow하고 test도 slow하다) proposal 단계에서 나온 2k의 box를 각각 학습하기 때문인데, 이를 Network로 해결하여, Fast R-CNN이 탄생하였다.

> Fast R-CNN

Fast R-CNN은 RP 단계에서 인풋을 ConvN에 넣어서 feature map을 만들고, 이 feature map을 이용하여 ROI를 산출한다. 나머지는 R-CNN과 마찬가지의 구조를 가지고 있다. 따라서 4개의 loss를가 존재하며, RPN의 오브젝트 구별과 box 정확도, 그리고 최종적인 물체 분류 정확도와 최종 box 정확도를 loss로 이용한다.

> Q. How to Regulaztion? (이해못함)

많은 연구가 되고 있는 분야로, 두개로 나눈뒤에 학습을 하는 방법도 있고, 여러가지를 종합하여 하나로 학습하는 방법이 있지만, 실질적으로 큰 개선이나 차이가 없는 편이고, 따라서 계산적 이득을 위하여 하나로 예측을 하려고 하는것이 좋다.

> Q. 어떻게 RPN을 학습시키는가?

자세한건 넘어가려고 한다. 다만 ROI 가 포함되면 ok, ROI가 너무 부족하면 fail로 간주한다.

> Q. RPN object loss는 어떻게 구하는가?

ROI가 object이면 Loss가 없고, Object가 아니면 Loss를 주는 방식을 이용한다.

### 4. grid method

> YOLO/SSD

YOLO(you only look ones)와 SSD(Single shot detection)이 비슷한 시기에 비슷한 풀이법으로 사용된 gird 방식은, 7x7 정도의 사각형으로 나눈뒤에, 이를 살짝 변형하여, 마르거나 뚱뚱하게 하여 기본이 되는 상자를 3개정도 선정, 이를 모든 7 by 7 그리드에 적용시켜 CNN을 돌리는 방법이다. 비교적으로 속도는 빠르고, 정확도가 살짝 낮은 편이다.

### 5. Aside : Object Detection + Captioning

> Dense Captioning

이미지에 존재하는 PR ROI혹은 ROI각각 마다 상황에 맞게 captioning을 하는 것이다.

# Instance Segmentation

다음 정보가 검출됩니다.

* (화면내 존재하는 모든 객체에 대하여) **bounding box**와 **라벨**
* **segmentation 정보** (단, background는 segmentation이 존재하지 않음)

> MASK R-CNN

R-CNN과 비슷한 특징을 가진 MASK R-CNN이 사용되는데, 이는 labeling에서 끝나는 R-CNN을 가져와 Class에 대한 mask를 예측하게 하는 다른점이 있습니다. MASK R-CNN은 그동안의 문제의 총괄적인 해결책으로 제시되었으면서, 많은 분야에서 성공적인 분류를 제시하고 있습니다.

그 예론
* GPU에서 5 frame 만에 계산됨
* 관중이 몇명이든 잘 찾아냄
* 사람의 pose를 잘 예측함 (심지어 가려져 있어도)
* 사람이 몇명이는 pose를 예측 할 수 있음

> Q. 어떤 데이터 셋이 쓰였나요?

COCO-dataset 이라는 microsoft사의 직원들이 joint를 모두 annotate한 데잍터가 있습니다. 200 thoasnd train set과 80 category 존재합니다.

신기한 점은, 적은 데이터로도 좋은 효과를 낸다는 것이며, 앞으로 몇 년간 데이터가 적어도 잘 작동하는 NN을 만들기 위한 연구가 진행될 것으로 보입니다.
